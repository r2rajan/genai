{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb1dd6d",
   "metadata": {},
   "source": [
    "### Lab 2- Facebook AI Similarity Search (FAISS) Index Partitioning\n",
    "\n",
    "Following up with Lab 1, when using large data sets, flat index and exhaustive sarch are not ideal for performing similarity search. This would require paritioning the index for performing efficient search. For paritioning let's introduce the concept of voronai diagrams. In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. A good example is let's take the country of Canada. Canada has many provinces. In a Voronai diagram, Canada can be split into 4 groups of provinces. Atlantic Canada, Eastern provinces, Prairies and Western provinces. Provinces Nova scotia, New Brunswick, New foundland labrador and Prince Edward Island. A Tourist visiting a procince insite Atlantic Canada has more probability to visit the other  provinces within Atlantic Canada than in the Prairies or western canada. The same principle applies to vector search considering equal distances between the results.\n",
    "\n",
    "Ithe previous lab, we used IndexFlatL2 index to build and store vectors. In this lab, you will use voronai cells to parition the index to optimize the performance of search results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# 1. This lab can requires some large datasets \n",
    "# 2. You will split document into sentences\n",
    "# 3. Create a new index and train it on the data\n",
    "# 4. Split the index into partitions of voronai cells\n",
    "# 5. Given a query, i.e. \"Who plays foot \", you find the K most similar sentences\n",
    "# 6. Adjust the \"k\" parameter to explore speed vs accuracy vs approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will the need python libraries for this tutorial. A basic understanding of python is required. \n",
    "# You can install the libraries using pip if not in your notebook pre-installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "# create dataframe\n",
    "data = pd.read_csv(StringIO(res.text), sep='\\t', on_bad_lines='skip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acb34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You take all the sentences from into a python list \n",
    "# You will get an output of 4.5K sentences\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentences[:5]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You take all samples from both sentence A and B and merge them together\n",
    "# You will get ~4.8K unique sentences\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentence_b = data['sentence_B'].tolist()\n",
    "sentences.extend(sentence_b)   \n",
    "len(set(sentences))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03072a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still the dataset is small. You are going to add more data by parsing the data from below URLS\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.train.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2013/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/headlines.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/belief.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/answers-students.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/answers-forums.test.tsv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these datasets have the same structure, so we loop through each creating our sentences data\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    # extract to dataframe\n",
    "    data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, on_bad_lines='skip')\n",
    "    # add to columns 1 and 2 to sentences list\n",
    "    sentences.extend(data[1].tolist())\n",
    "    sentences.extend(data[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the data by removin duplicates and NaN \n",
    "# You will get approximately 25k sentences\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74187500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install sentence_transformers library. This framework provides an easy method to compute \n",
    "# dense vector representations for sentences, paragraphs, and images.\n",
    "# For additional reading https://pypi.org/project/sentence-transformers/\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. \n",
    "# and achieve state-of-the-art performance in various task. Read the pypi library link about supported models. \n",
    "# You need to initialize sentence transformer model. \n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "# create sentence embeddings using the multi-qa-MiniLM-L6 model from hugging face\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e15a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the dimensions \n",
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndexIVFFlat - Inverted File index. Inverted file index takes two parameters\n",
    "# nlist : The number of clusters to be formed. These clusters are the voronai cells\n",
    "# quantizer : to assign the vectors to a particular cluster. This is usually another index that uses the L2 Euclidian distance metric (we use the IndexFlatL2 index)\n",
    "nlist = 50\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the index is trained. In the previous IndexFlatL2 training was not required and it will return true\n",
    "# When using voronai cells in Inverted file index, training the cells is required. The function below will return false.\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the index and check if index is now trained. It should return \"True\"\n",
    "index.train(sentence_embeddings)\n",
    "index.is_trained  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the vectors in to the index\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fa739",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e65da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now that our index is trained, let's query the index\n",
    "# \"xq\" Query vector\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# \"k\" specifies the number of similar vectors to be returned from the visited clusters.\n",
    "#Then search with a given query `xq` and number of nearest neigbors to return `k`.\n",
    "index.nprobe=2\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will be get 4 nearest locations returned by the query. Along with this you will know how long it takes to return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results of query and 4 nearest neighbours related to Jeff Bezos and Internet\n",
    "for i,location in enumerate(I[0].tolist()):\n",
    "    print(location, \":\", sentences[location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=4\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=8\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98333e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=16\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are searching large number of clusters in order of 2, 4,8 and 16 and still receive responses faster than IndexFlatL2-only index \n",
    "# Is this the final step of optimization and can we optimize the performance further\n",
    "# Next Step: Product quantization "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
