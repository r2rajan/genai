{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f60949f",
   "metadata": {},
   "source": [
    "### Lab - Facebook AI Similarity Search (FAISS) \n",
    "\n",
    "In this notebook, we'll learn about Facebook AI Similarity Search. Facebook released Facebook AI Similarity Search (Faiss) library in March' 2017. FAISS library allows to search multimedia documents that are similar to each other where query based search enginess fall short.  \n",
    "\n",
    "###  What is a vector database\n",
    "Traditional databases are made up of structured tables containing symbolic information. For example, a collection of images is represented as a table. Each row in the table contains the image identifier and image description. \n",
    "\n",
    "A vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes.  The vectors are usually generated by applying some kind of transformation or embedding function to the raw data, such as text, images, audio, video, and others. The embedding function can be based on various methods, such as machine learning models, word embeddings, feature extraction algorithms. \n",
    "\n",
    "AI tools, like text embedding (word2vec) or convolutional neural net (CNN) descriptors trained with deep learning, generate high-dimensional vectors.\n",
    "\n",
    "### How to Vector representation?\n",
    "The vector representation for images is designed to produce similar vectors for similar images, where similar vectors are defined as those that are nearby in Euclidean space. \n",
    "\n",
    "### Benefits of Vector Database\n",
    "The main advantage of a vector database is that it allows for fast and accurate similarity search and retrieval of data based on their vector distance or similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this lab, we will use FAISS to generate vectors \n",
    "# 1. We will use Amazon_Shareholder_Letter_1997.txt as the input document \n",
    "# 2. We will split document into sentences\n",
    "# 3. Create a new index and train it on the data\n",
    "# 4. Given a query, i.e. \"What did Jeff Bezos say about the internet?\", find the K most similar sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03fadd",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "\n",
    "- https://www.pinecone.io/learn/faiss-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8fb4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will the need python libraries for this tutorial. A basic understanding of python is required. \n",
    "# You can install the libraries using pip if not in your notebook pre-installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://raw.githubusercontent.com/r2rajan/genai/main/FAISS/Amazon_Shareholder_Letter_1997.txt')\n",
    "# create dataframe\n",
    "data = pd.read_csv(StringIO(res.text), sep='\\t', on_bad_lines='skip', header=None, names=['Sentences'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acb34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we take all the sentences from the Amazon Shareholder letter into a python list \n",
    "# you will get an output of 42 sentences\n",
    "sentences = data['Sentences'].tolist()\n",
    "sentences[:5]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences from Amazon Shareholder letter\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates and NaN\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74187500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install sentence_transformers library. This framework provides an easy method to compute \n",
    "# dense vector representations for sentences, paragraphs, and images.\n",
    "# For additional reading https://pypi.org/project/sentence-transformers/\n",
    "!pip install sentence-transformers\n",
    "import sentence_tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. \n",
    "# and achieve state-of-the-art performance in various task. Read the pypi library link about supported models. \n",
    "# You need to initialize sentence transformer model. \n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "# create sentence embeddings using the multi-qa-MiniLM-L6 model from hugging face\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e15a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the dimensions \n",
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build our first vector index using Indexflat L2\n",
    "# IndexFlatL2 measures the L2 (or Euclidean) distance between all given points between our query vector(xq), and the vectors(y) loaded into the index. \n",
    "# Itâ€™s simple, accurate, but not fast. \n",
    "# You want the index to have the same dimension as your emmeddings\n",
    "index = faiss.IndexFlatL2(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the index is trained. IndexFlatL2 training is not required and it will return true\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b1cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load your sentence embeddings in to the index\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fa739",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e65da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query = xq\n",
    "#vectors to return = k\n",
    "#Then search with a given query `xq` and number of nearest neigbors to return `k`.\n",
    "k = 4\n",
    "xq = model.encode([\"What did Bezos say about internet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will be get 4 nearest locations returned by the query. Along with this you will know how long it takes to return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results of query and 4 nearest neighbours related to Jeff Bezos and Internet\n",
    "for i,location in enumerate(I[0].tolist()):\n",
    "    print(location, \":\", sentences[location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have 4 vectors to return (k) - so we initialize a zero array to hold them\n",
    "vecs = np.zeros((k, d))\n",
    "# then iterate through each location ID from I and reconstruct the vector from the index \n",
    "# Add the reconstructed vector to our zero-array\n",
    "for i, val in enumerate(I[0].tolist()):\n",
    "    vecs[i, :] = index.reconstruct(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look the shape of the numpy array \n",
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the actual vectors of our result. \n",
    "vecs[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a520eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's the end of this simple lab to explore vector databases and vector search\n",
    "# You used a simple flat index and did a exhaustive search on a very small dataset. \n",
    "# A flat index is not ideal for very large datasets with billions of parameters where performance is key\n",
    "# Next Steps: How to improve the performance by partitioning the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f3169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
