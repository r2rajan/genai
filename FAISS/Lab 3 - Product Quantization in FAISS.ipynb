{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb1dd6d",
   "metadata": {},
   "source": [
    "### Lab 3 - Product quantization in FAISS\n",
    "\n",
    "Following up with Lab 2, we were using flat index and full vectors which soon can become a huge problem in ver large datasets. So you need a solution to compress the vectors. This is akin to compressing a file to save disk space.  FAISS provides an optimization technique called Product Quantization (PQ). Product Quantization in vector search allows to speed up results.\n",
    "\n",
    "Let's use the example of Canada to illustrate Product Quantization. Each candian province is divided into multiple municipal wards. Groups of municipal wards are clustered to form either a city or town or rural municipality. The clustered group of wards is represented by a city council or townsmen or rural municipal council. In the same vein, we divide vectors into sub-vectors. The sub-vectors are clustered into centroids. The vector of sub-vector is represented by the nearest ID of the centroid. \n",
    "\n",
    "In the previous lab we used IndexIVFflat for index patitioning. In this lab we will use IndexIVFPQ index for product quantization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. This lab cannot be run on its own you should have com\n",
    "# 2. You will split document into sentences\n",
    "# 3. Create a new index and train it on the data\n",
    "# 4. Split the index into partitions of voronai cells\n",
    "# 5. Given a query, i.e. \"Who plays foot \", you find the K most similar sentences\n",
    "# 6. Adjust the \"k\" parameter to explore speed vs accuracy vs approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99d0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will the need python libraries for this tutorial. A basic understanding of python is required. \n",
    "# You can install the libraries using pip if not in your notebook pre-installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd7a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e37053",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/sick2014/SICK_train.txt')\n",
    "# create dataframe\n",
    "data = pd.read_csv(StringIO(res.text), sep='\\t', on_bad_lines='skip')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acb34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You take all the sentences from into a python list \n",
    "# You will get an output of 4.5K sentences\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentences[:5]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79dbd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You take all samples from both sentence A and B and merge them together\n",
    "# You will get ~4.8K unique sentences\n",
    "sentences = data['sentence_A'].tolist()\n",
    "sentence_b = data['sentence_B'].tolist()\n",
    "sentences.extend(sentence_b)   \n",
    "len(set(sentences))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03072a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still the dataset is small. You are going to add more data by parsing the data from below URLS\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.train.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/MSRpar.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2012/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2013/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/OnWN.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2014/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/images.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/headlines.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/belief.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/answers-students.test.tsv',\n",
    "    'https://raw.githubusercontent.com/brmson/dataset-sts/master/data/sts/semeval-sts/2015/answers-forums.test.tsv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these datasets have the same structure, so we loop through each creating our sentences data\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    # extract to dataframe\n",
    "    data = pd.read_csv(StringIO(res.text), sep='\\t', header=None, on_bad_lines='skip')\n",
    "    # add to columns 1 and 2 to sentences list\n",
    "    sentences.extend(data[1].tolist())\n",
    "    sentences.extend(data[2].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2860fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean up the data by removin duplicates and NaN \n",
    "# You will get approximately 25k sentences\n",
    "sentences = [word for word in list(set(sentences)) if type(word) is str]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74187500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install sentence_transformers library. This framework provides an easy method to compute \n",
    "# dense vector representations for sentences, paragraphs, and images.\n",
    "# For additional reading https://pypi.org/project/sentence-transformers/\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b407b448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. \n",
    "# and achieve state-of-the-art performance in various task. Read the pypi library link about supported models. \n",
    "# You need to initialize sentence transformer model. \n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "# create sentence embeddings using the multi-qa-MiniLM-L6 model from hugging face\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e15a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the dimensions \n",
    "d = sentence_embeddings.shape[1]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IndexIVFPQ- Inverted file with Product Quantizer encoding. \n",
    "# Each residual vector is encoded as a product quantizer code\n",
    "# nlist : The number of clusters to be formed. These clusters are the voronai cells\n",
    "# quantizer : to assign the vectors to a particular cluster. This is usually another index that uses the L2 Euclidian distance metric (we use the IndexFlatL2 index)\n",
    "# k represents the total number of centroids (or codes) that will be used to represent our vectors. \n",
    "# m represents the number of subvectors that we will split our vectors into. \n",
    "nlist = 50\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "k = 8 #Note: The dimension has to be a multiple of k\n",
    "m = 8\n",
    "index = faiss.IndexIVFPQ(quantizer, d, nlist, k, m, faiss.METRIC_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab98b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if the index is trained. \n",
    "# When using product quantizer in Inverted file index, training the cells is required. The function below will return false.\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train the index and check if index is now trained. It should return \"True\"\n",
    "index.train(sentence_embeddings)\n",
    "index.is_trained  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee77e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the vectors in to the index\n",
    "index.add(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7fa739",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e65da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now that our index is trained, We are ready to search our index\n",
    "# \"xq\" Query vector\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# \"k\" specifies the number of similar vectors to be returned from the visited clusters.\n",
    "#Then search with a given query `xq` and number of nearest neigbors to return `k`.\n",
    "index.nprobe=2\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e2b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You will be get 4 nearest locations returned by the query. Along with this you will know how long it takes to return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the results of query and 4 nearest neighbours related to Jeff Bezos and Internet\n",
    "for i,location in enumerate(I[0].tolist()):\n",
    "    print(location, \":\", sentences[location])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47a9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=4\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=8\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98333e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# \"nprobe\" parameter specifies the number of clusters to visit during the search operation\n",
    "# Let's increase the scope of clusters to search \n",
    "index.nprobe=16\n",
    "k = 4\n",
    "xq = model.encode([\"Who is playing football\"])\n",
    "D, I = index.search(xq, k)  # search\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9588f55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this lab, you learnt how you can optimize further using product quantization. \n",
    "# You should see 0 to 30% improvement in search speed \n",
    "# You would have noticed the one of the result location obtained was different from IndexIVFFlat index.\n",
    "# This is due to an additional layer of approximation introduced to gain speed.\n",
    "# \n",
    "\n",
    "# Next Step: Perform a comparison of results in this notebook using different optimization techniques. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
